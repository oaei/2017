<html><head><title>Ontology Alignment Evaluation Initiative::Conference track|evaluation</title>

<meta http-equiv="Content-type" content="text/html; charset=utf-8">
<script type="text/javascript" src="http://code.jquery.com/jquery-latest.min.js"></script>
<link rel="stylesheet" type="text/css" href="http://cdn.datatables.net/1.10.2/css/jquery.dataTables.css">
<script type="text/javascript" language="javascript" src="http://cdn.datatables.net/1.10.2/js/jquery.dataTables.min.js"></script>
<link rel="stylesheet" type="text/css" href="../style.css">
<style type="text/css" class="init">
</style>
<script type="text/javascript" class="init">
    $(document).ready(function() {
	$('#tab').dataTable({
	  "paging":   false,
	  "info":     false
        });
    } );
</script>
</head><body>

<div class="header">
<a style="color: grey; line-height: 5mm;" href="http://oaei.ontologymatching.org/2017/">Ontology
  Alignment Evaluation Initiative - OAEI-2017
  Campaign</a><a href="http://oaei.ontologymatching.org/"><img src="../../oaeismall.jpg" alt="OAEI" style="border-style: none; float: right; margin-left: 5pt;"></a>
  <img  width="200px" src="../../seals-logo.jpg" alt="OAEI" style="clear:right;float:right; margin-left: 5pt; border-style:none;"/></a>
</div>

<!-- todo sepsat list of participants? -->

<h1>Results of Evaluation for the Conference track within OAEI 2017</h1>

<p>
Web page content
</p>
<ul>
<li><a href="#participants">Participants</a>
<li><a href="#data">Data</a>
<li><a href="#modalities">Evaluation modalities</a>
<li><a href="#crisp-ra">Evaluation based on crisp reference alignments</a>
<ul>
<li><a href="#setting">Evaluation setting and tables description</a>
<li><a href="#ref-comparison">Comparison of OAEI 2016 to 2017</a>
<li><a href="#visualization">Results visualization on precision/recall triangular graph for rar2-M3</a>
<li><a href="#discussion">Discussion for evaluation based on crisp reference alignments</a>
</ul>
<li><a href="#uncertain-ra">Evaluation based on the uncertain version of the reference alignment</a></h3>
<ul>
<li><a href="#uncsetting">Evaluation setting</a>
<li><a href="#uncresults">Results</a>
<li><a href="#uncdiscussion">Discussion for evaluation based on the uncertain reference alignments</a>
</ul>
<li><a href="#logical">Evaluation based on logical reasoning</a>
<li><a href="#organizers">Organizers</a>
<li><a href="#references">References</a>
</ul>

<h2><a name="participants">Participants</h2>
<p>
This year we have 10 participants (ALIN, AML, KEPLER, LogMap, LogMapLt, ONTMAT, POMap, SANOM, WikiV3 and XMap) which managed to generate meaningful output. Other matchers did not generate any alignments (those matchers focusing on instance matching or on biomedical ontologies) except LogMapBio which output is almost identical to LogMap for this track; we only used LogMap. 
<!-- For overview table please see <a href="../results/index.html">general information about results</a>.  -->
We also provide comparison with tools that participated in previous years of OAEI in terms of highest average F1-measure.
</p>

<h2><a name="data">Data</a></h2>

<h3>Participants alignments</h3>

<p>You can <a href="data/conference2017-results.zip">download</a> a subset of all alignments for which there is a reference alignment. In this case we provide alignments as generated by the SEALS platform (afterwards we applied some tiny modifications which we explained below). Alignments are stored as it follows: matcher-ontology1-ontology2.rdf.</p>

<h2><a name="modalities">Evaluation modalities</a></h2>
<p>
Tools have been evaluated based on</p>
<ul>
<li><a href="#crisp-ra">crisp reference alignments</a> where the confidence values for all matches are 1.0.
<li><a href="#uncertain-ra">the uncertain version of the reference alignment</a> where confidence values reflect the degree of agreement of a group of twenty people on the validity of the match [1], 
<li><a href="#logical">logical reasoning</a> using violations of consistency and conservativity principles (presented at OWLED 2014 and at ISWC 2014 [2,3]). 
</ul>

<h3><a name="crisp-ra">Evaluation based on crisp reference alignments</a></h3>
<p>
We have three variants of crisp reference alignments (the confidence values for all matches are 1.0). They contain 21 alignments (test cases), which corresponds to the complete alignment space between 7 ontologies from the OntoFarm data set. This is a subset of all ontologies within this track (16) [4], see <a href="http://owl.vse.cz:8080/ontofarm/">OntoFarm data set web page</a>.
</p>
<ul>
<li><b>ra1</b> is the original reference alignment which can be <a href="data/reference-alignment.zip">downloaded</a> - please let us know how you use this reference-alignment (outside the OAEI context) and data set (ondrej.zamazal at vse dot cz).
<li><b>ra2</b> is an entailed reference alignment (ra2) generated as a transitive closure computed on the original reference alignment (ra1). In order to obtain coherent reference alignment set,  conflicting correspondences have been inspected and resolved by evaluators. As a result the degree of correctness and completeness of ra2 is probaly slightly better than for ra1. However, the differences are relatively restricted.
<li><b>rar2</b> is a violation free version of reference alignment (ra2). First violating correspondences have been detected using approach from [2, 3] and then carefully resolved by an evaluator.
</ul>

<p>For each reference alignment we provide three evaluation variants</p>
<ul>
<li><b>M1</b> only contains classes,
<li><b>M2</b> only contains properties,
<li><b>M3</b> contains classes and properties, i.e. M3 = M1 + M2.
</ul>

<p>rar2 M3 is used as the main reference alignment for this year. It will also be used within the synthesis paper.</p>

<table border="1">
<tr><td></td><td>ra1</td><td>ra2</td><td>rar2</td></tr>
<tr><td>M1</td><td><a href="#ra1-M1">ra1-M1</a></td><td><a href="#ra2-M1">ra2-M1</a></td><td><a href="#rar2-M1">rar2-M1</a></td></tr>
<tr><td>M2</td><td><a href="#ra1-M2">ra1-M2</a></td><td><a href="#ra2-M2">ra2-M2</a></td><td><a href="#rar2-M2">rar2-M2</a></td></tr>
<tr><td>M3</td><td><a href="#ra1-M3">ra1-M3</a></td><td><a href="#ra2-M3">ra2-M3</a></td><td><a href="#rar2-M3">rar2-M3</a></td></tr>
</table>

<h4><a name="setting">Evaluation setting and tables description</a></h4>
<p>
Regarding evaluation based on reference alignment, we first filtered out (from alignments generated using SEALS platform) all instance-to-any_entity and owl:Thing-to-any_entity correspondences prior to computing Precision/Recall/F1-measure/F2-measure/F0.5-measure because they are not contained in the reference alignment. 
<!-- Next, we filter out generated alignments with "null" entities and convert confidence values higher than 1.0 to certain confidence value.  -->
In order to compute average Precision and Recall over all those alignments we used absolute scores (i.e. we computed precision and recall using absolute scores of TP, FP, and FN across all 21 test cases). This corresponds to micro average precision and recall. Therefore, the resulting numbers can slightly differ with those computed by the SEALS platform (macro average precision and recall). Then, we computed F1-measure in a standard way. Finally, we found the highest average F1-measure with thresholding (if possible).
</p>

<p>
In order to provide some context for understanding matchers performance we included two simple string-based matchers as baselines. StringEquiv (before it was called Baseline1) is a string matcher based on string equality applied on local names of entities which were lowercased before (this baseline was also used within anatomy track 2012) and edna (string editing distance matcher) was adopted from benchmark track (wrt. performance it is very similar to the previously used baseline2).
</p>

<p>In the tables below, there are results of all 10 tools
with regard to all combinations of evaluation variants with crisp reference alignments. There are precision, recall, F1-measure, F2-measure and F0.5-measure computed for the threshold that provides the highest average F1-measure computed for each matcher.  
F1-measure is the harmonic mean of precision and recall. F2-measure (for beta=2) weights recall higher than precision and F0.5-measure (for beta=0.5) weights precision higher than recall.
</p>

<!-- <p>Tools are ordered according to their highest average F1-measure. According to matcher's position with regard to two baselines it can be in one of three basic groups:</p> -->

<h4><a name="ra1-M1">ra1-M1</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="ra1-M1.png" alt="ra1-M1">

<h4><a name="ra1-M2">ra1-M2</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="ra1-M2.png" alt="ra1-M2">

<h4><a name="ra1-M3">ra1-M3</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="ra1-M3.png" alt="ra1-M3">

<h4><a name="ra2-M1">ra2-M1</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="ra2-M1.png" alt="ra2-M1">

<h4><a name="ra2-M2">ra2-M2</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="ra2-M2.png" alt="ra2-M2">

<h4><a name="ra2-M3">ra2-M3</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="ra2-M3.png" alt="ra2-M3">

<h4><a name="rar2-M1">rar2-M1</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="rar2-M1.png" alt="rar2-M1">

<h4><a name="rar2-M2">rar2-M2</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="rar2-M2.png" alt="rar2-M2">

<h4><a name="rar2-M3">rar2-M3</a></h4>
<p><a href="#crisp-ra">[back to explanation]</a></p>

<img src="rar2-M3.png" alt="rar2-M3">

<h4><a name="ref-comparison">Comparison of OAEI 2017 and 2016</a></h4>
<p>Table below summarizes performance results of tools participated last 2 years of OAEI, conference track with regard to reference alignment rar2.</p>
<img src="comparison1-rar2-M3.png" alt="Perfomance results summary OAEI 2017 and 2016">
<p>
Based on this evaluation we can see that five matching tools participated in the previous year did not change the results.
<!-- for the synthesis paper I will only use ra2 and in comparison with others -->
</p>

<img src="comparison2-rar2-M3.png" alt="Difference between 2017 and 2017 results">

<h4><a name="visualization">Results visualization on precision/recall triangular graph based on the rar2-M3 reference alignment</a></h4>
<p>
All tools except ONTMAT (precision and recall were below 0.5) are visualized in terms of their performance regarding an average F1-measure in the figure below. Tools are represented as squares or triangles. Baselines are represented as circles. Horizontal line depicts level of precision/recall while values of average F1-measure are depicted by areas bordered by corresponding lines F1-measure=0.[5|6|7].
</p>
<img src="triangular-rar2-M3.png" alt="precision/recall triangular graph for conference and F1-measure based on rar2-M3">

<h4><a name="discussion">Discussion for evaluation based on crisp reference alignments</a></h4>
<p>
With regard to two baselines we can group tools according to matcher's position (above best edna baseline, above StringEquiv baseline, below StringEquiv baseline). There is not a difference among ra1-M3, ra2-M3 and rar2-M3 regarding tools position. LogMap has the largest drop (by 0.06 of F-Measure) between ra2-M3 and ra1-M3. In all there are four tools above edna baseline (AML, LogMap, XMap and LogMapLt) and further two tools performed better than StringEquiv (KEPLER and WikiV3) for rar2 reference alignment which is considered as most correct and most difficult reference alignment. Since rar2 is not only consistency violation free (as ra2) but also conservativity violation free we consider the rar2 as main reference alignment for this year. It will also be used within the synthesis paper. 
</p>
<p>
Based on the evaluation variants M1 and M2 it can be seen that there are four tools (ALIN, POMap, ONTMAT and SANOM) which do not match properties at all. Of course, this has a negative effect on overall tools performance within the M3 evaluation variant.
</p>

<h3><a name="uncertain-ra">Evaluation based on the uncertain version of the reference alignment</a></h3>
<h4><a name="uncsetting">Evaluation setting</a></h4>
<p>The confidence values of all matches in the standard (sharp) reference alignments for the conference track are all 1.0. For the uncertain version of this track, the confidence value of a match has been set equal to the percentage of a group of people who agreed with the match in question (this uncertain version is based on reference alignment labeled ra1). One key thing to note is that the group was only asked to validate matches that were already present in the existing reference alignments - so some matches had their confidence value reduced from 1.0 to a number near 0, but no new matches were added</p>
<p>There are two ways that we can evaluate alignment systems according to these 'uncertain' reference alignments, which we refer to as <b>discrete</b> and <b>continuous</b>. The discrete evaluation considers any match in the reference alignment with a confidence value of 0.5 or greater to be fully correct and those with a confidence less than 0.5 to be fully incorrect. Similarly, an alignment system’s match is considered a 'yes' if the confidence value is greater than or equal to the system’s threshold and a 'no' otherwise. In essence, this is the same as the 'sharp' evaluation approach, except that some matches have been removed because less than half of the crowdsourcing group agreed with them. The continuous evaluation strategy penalizes an alignment system more if it misses a match on which most people agree than if it misses a more controversial match. For instance, if A = B with a confidence of 0.85 in the reference alignment and an alignment algorithm gives that match a confidence of 0.40, then that is counted as 0.85 * 0.40 = 0.34 of a true positive and 0.85 – 0.40 = 0.45 of a false negative.</p>

<h4><a name="uncresults">Results</a></h4>
<p>Below is a graph showing the F-measure, precision, and recall of the different alignment systems when evaluated using the sharp (s), discrete uncertain (d) and continuous uncertain (c) metrics, along with a table containing the same information. The results from this year show that more systems are assigning nuanced confidence values to the matches they produce.</p>
<img src="uncertainGraph.png" alt="graph for uncertain reference alignment based evalation">
<img src="uncra1-M3.png" alt="results for uncertain reference alignment based evalation">

<p>Out of the ten alignment systems, three (ALIN, LogMapLt and ONTMAT) use 1.0 as the confidence value for all matches they identify. Two more have a narrow range of confidence values (POMap’s values vary between 0.8 and 1.0, with the majority falling between 0.93 and 1.0 while SANOM’s values are relatively tightly clustered between 0.73 and 0.9). The remaining five systems (AML, KEPLER, LogMap, WikiV3 and XMap) have a wide variation of confidence values.</p>

<h4><a name="uncdiscussion">Discussion for evaluation based on the uncertain reference alignments</a></h4>
<p>
When comparing the performance of the matchers on the uncertain reference alignments versus that on the sharp version, we see that in the discrete case all matchers performed the same or better in terms of F-measure. Improvement in F-measure ranged from 0 to 8 percent over the sharp reference alignment. This was driven by increased recall, which is a result of the presence of fewer 'controversial' matches in the uncertain version of the reference alignment.
</p>

<p>
The performance of most matchers is very similar regardless of whether a discrete or continuous evaluation methodology is used (provided that the threshold is optimized to achieve the highest possible F-measure in the discrete case). The primary exceptions to this are KEPLER, LogMap and SANOM. These systems perform significantly worse when evaluated using the continuous version of the metrics. In the LogMap and SANOM cases, this is because the matcher assigns low confidence values to some matches in which the labels are equivalent strings, which many crowdsourcers agreed with unless there was a compelling technical reason not to. This hurts recall, but using a low threshold value in the discrete version of the evaluation metrics 'hides' this problem. In the case of KEPLER, the issue is that entities whose labels share a word in common have fairly high confidence values, even though they are often not equivalent. For example, Review and Reviewing_Event. This hurts precision in the continuous case, but is taken care of by using a high threshold value in the discrete case.
</p>

<p>
Five systems from this year also participated last year, and thus we are again able to make some comparisons over time. The F-measures of all systems essentially held constant (within one percent) when evaluated against the uncertain reference alignments. This is in contrast to last year, in which most systems made modest gains (in the neighborhood of 1 to 6 percent) over 2015. It seems that, barring any new advances, participating systems have reached something of a steady state on this performance metric.
</p>

<!--<p>Perhaps more importantly, the difference in the performance of most systems between the discrete and continuous evaluation has shrunk between this year and last. This is an indication that more systems are providing confidence values that reflect the disagreement of humans on various matches.</p>-->

<h3><a name="logical">Evaluation based on logical reasoning</a></h3>
<p>
For evaluation based on logical reasoning we applied detection of conservativity and consistency principles violations [2, 3]. While consistency principle proposes that correspondences should not lead to unsatisfiable classes in the merged ontology, conservativity principle proposes that correspondences should not introduce new semantic relationships between concepts from one of input ontologies [2].
</p>
<p>
Table below summarizes statistics per matcher. There are number of all alignments (#Align.), number of alignments (#Incoh.Align.) that cause unsatisfiable TBox after ontologies merge, total number of all conservativity principle violations within all alignments (#TotConser.Viol.) and its average per one alignment (#AvgConser.Viol.), total number of all consistency principle violations (#TotConsist.Viol.) and its average per one alignment (#AvgConsist.Viol.).
</p>
<p>
Five tools (ALIN, AML, LogMap, ONTMAT and POMap) have no consistency principle violation (in comparison to seven last year) and two tools (SANOM and XMap) generated only one incoherent alignment. There is one tool (ALIN) having no conservativity principle violations. Further two tools have average of conservativity principle around 1 (ONTMAT and POMap). We should note that these conservativity principle violations can be "false positives" since the entailment in the aligned ontology can be correct although it was not derivable in the single input ontologies.
</p>

<table id="tab" class="display" cellspacing="0" width="100%">
  <thead>
   <tr>
    <th>Matcher</th>
    <th>#Align.</th>
    <th>#Incoh.Align.</th>
    <th>#TotConser.Viol.</th>
    <th>#AvgConser.Viol.</th>
    <th>#TotConsist.Viol.</th>
    <th>#AvgConsist.Viol.</th>
   </tr>
  </thead>
  <tbody>
   <tr>
    <td>ALIN</td>
    <td>21</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
    <td>0</td>
   </tr>
   <tr>
    <td>AML</td>
    <td>21</td>
    <td>0</td>
    <td>39</td>
    <td>1.86</td>
    <td>0</td>
    <td>0</td>
   </tr>
   <tr>
    <td>KEPLER</td>
    <td>21</td>
    <td>12</td>
    <td>123</td>
    <td>5.86</td>
    <td>159</td>
    <td>7.57</td>
   </tr>
   <tr>
    <td>LogMap</td>
    <td>21</td>
    <td>0</td>
    <td>25</td>
    <td>1.19</td>
    <td>0</td>
    <td>0</td>
   </tr>
   <tr>
    <td>LogMapLt</td>
    <td>21</td>
    <td>5</td>
    <td>96</td>
    <td>4.57</td>
    <td>25</td>
    <td>1.19</td>
   </tr>
   <tr>
    <td>ONTMAT</td>
    <td>20</td>
    <td>0</td>
    <td>1</td>
    <td>0.05</td>
    <td>0</td>
    <td>0</td>
   </tr>
   <tr>
    <td>POMap</td>
    <td>21</td>
    <td>0</td>
    <td>1</td>
    <td>0.05</td>
    <td>0</td>
    <td>0</td>
   </tr>
   <tr>
    <td>SANOM</td>
    <td>21</td>
    <td>1</td>
    <td>11</td>
    <td>0.52</td>
    <td>18</td>
    <td>0.86</td>
   </tr>
   <tr>
    <td>WikiV3</td>
    <td>21</td>
    <td>10</td>
    <td>125</td>
    <td>5.95</td>
    <td>58</td>
    <td>2.76</td>
   </tr>
   <tr>
    <td>XMap</td>
    <td>21</td>
    <td>1</td>
    <td>22</td>
    <td>1.05</td>
    <td>4</td>
    <td>0.19</td>
   </tr>
  </tbody>
</table>

<h4>Statistics of consistency and conservativity principle violations</h4>

<p>Here we list ten most frequent unsatisfiable classes appeared after ontologies merge by any tool. Five tools generated incoherent alignments.</p>

<pre>
ekaw#Rejected_Paper - 4
ekaw#Contributed_Talk - 4
ekaw#Camera_Ready_Paper - 4
ekaw#Accepted_Paper - 4
sigkdd#Conference - 3
sigkdd#ACM_SIGKDD - 3
iasted#Worker_non_speaker - 3
iasted#Student_registration_fee - 3
iasted#Student_non_speaker - 3
iasted#Nonauthor_registration_fee - 3
</pre>

<!--<p>Here we list ten most frequent unsatisfiable classes appeared after ontologies merge by ontology pairs. These unsatisfiable classes were appeared in all ontology pairs for given ontology:</p>

<pre>
ekaw#Contributed_Talk - 4
edas#Reviewer - 4
ekaw#Rejected_Paper - 3
ekaw#Invited_Speaker - 3
ekaw#Evaluated_Paper - 3
ekaw#Camera_Ready_Paper - 3
ekaw#Accepted_Paper - 3
conference#Invited_speaker - 3
cmt#PaperAbstract - 3
sigkdd#Author_of_paper_student - 2
</pre>-->

<p>Here we list ten most frequent caused new semantic relationships between concepts within input ontologies by any tool:</p>

<pre>
iasted#Record_of_attendance, http://iasted#City - 8
	edas-iasted
conference#Invited_speaker, http://conference#Conference_participant - 7
	conference-sigkdd
	conference-ekaw
iasted#Video_presentation, http://iasted#Item - 4
	conference-iasted
	edas-iasted
iasted#Video_presentation, http://iasted#Document - 4
	conference-iasted
iasted#Sponzorship, http://iasted#Registration_fee - 4
	iasted-sigkdd
iasted#Sponzorship, http://iasted#Fee - 4
	iasted-sigkdd
iasted#Session_chair, http://iasted#Speaker - 4
	ekaw-iasted
	iasted-sigkdd
iasted#Presentation, http://iasted#Item - 4
	conference-iasted
	edas-iasted
iasted#Presentation, http://iasted#Document - 4
	conference-iasted
iasted#PowerPoint_presentation, http://iasted#Item - 4
	conference-iasted
	edas-iasted
</pre>

<!--<p>Here we list ten most frequent caused new semantic relationships between concepts within input ontologies by ontology pairs:</p>

<pre>
iasted#Video_presentation, http://iasted#Item - 2
	LogMap
	KEPLER
	AML
	XMap
iasted#Speaker, http://iasted#Author - 2
	WikiV3
	KEPLER
	LogMapLt
iasted#Session_chair, http://iasted#Speaker - 2
	WikiV3
	LogMap
	KEPLER
	LogMapLt
iasted#Presentation, http://iasted#Item - 2
	LogMap
	KEPLER
	AML
	XMap
iasted#PowerPoint_presentation, http://iasted#Item - 2
	LogMap
	KEPLER
	AML
	XMap
ekaw#Tutorial_Abstract, http://ekaw#Paper - 2
	KEPLER
	AML
ekaw#Invited_Talk_Abstract, http://ekaw#Paper - 2
	KEPLER
	AML
ekaw#Abstract, http://ekaw#Paper - 2
	KEPLER
	AML
conference#Reviewed_contribution, http://conference#Regular_contribution - 2
	KEPLER
	AML
conference#Rejected_contribution, http://conference#Regular_contribution - 2
	KEPLER
	AML
</pre>-->

<h2><a name="organizers">Organizers</a></h2>
<ul>
<li>Ondřej Zamazal (University of Economics, Prague), main contact for the track, ondrej dot zamazal at vse dot cz
<li>Michelle Cheatham (Wright State University, USA)
<!-- <li>Alessandro Solimando (Universita di Genova, Italy) -->
</ul>

<p>We would like to thank to Alessandro Solimando for his help with running the code for logical reasoning based evaluation.</p>

<h2><a name="references">References</a></h2>

<p>[1] Michelle Cheatham, Pascal Hitzler: Conference v2.0: An Uncertain Version of the OAEI Conference Benchmark. International Semantic Web Conference (2) 2014: 33-48.</p>
<p>[2] Alessandro Solimando, Ernesto Jiménez-Ruiz, Giovanna Guerrini:
Detecting and Correcting Conservativity Principle Violations in Ontology-to-Ontology Mappings. International Semantic Web Conference (2) 2014: 1-16.</p>
<p>[3] Alessandro Solimando, Ernesto Jiménez-Ruiz, Giovanna Guerrini: A Multi-strategy Approach for Detecting and Correcting Conservativity Principle Violations in Ontology Alignments. OWL: Experiences and Directions Workshop 2014 (OWLED 2014). 13-24.
</p>
<p>[4] Ondřej Zamazal, Vojtěch Svátek. The Ten-Year OntoFarm and its Fertilization within the Onto-Sphere. Web Semantics: Science, Services and Agents on the World Wide Web, 43, 46-53. 2017.
</p>


<!--
<p>
-->
<!--[1] Šváb O., Svátek V. Combining Ontology Mapping Methods Using Bayesian Networks. OM-2006 at ISWC-2006. -->
<!--
</p>
<p>
[1] Scharffe F., Euzenat J., Ding Y., Fensel,D. Correspondence patterns for ontology mediation. OM-2007 at ISWC-2007.</p>
<p>
[2] Šváb O., Svátek V., Stuckenschmidt H.: A Study in Empirical and 'Casuistic' Analysis of Ontology Mapping Results. ESWC-2007.
<a href="http://nb.vse.cz/%7Esvatek/abstr.htm#eswc07" target="_blank">Abstract</a>
<a href="http://nb.vse.cz/%7Esvatek/eswc07su.pdf" target="_blank">Draft paper</a> (final version available via <a href="http://www.springerlink.com/content/l3327x8n451g12r0/?p=1b0e634a25944c04836ca09588855c3d&amp;pi=24">SpringerLink</a>)
</p>
<p>
[3] Meilicke C., Stuckenschmidt H. Incoherence as a basis for measuring the quality of ontology mappings. OM-2008 at ISWC 2008.
</p>
<p>
[4] van Hage W.R., Isaac A., Aleksovski Z. Sample evaluation of ontology matching systems. EON-2007, Busan, Korea, 2007.
</p>
<p>
[5] Šváb-Zamazal O., Svátek V. Empirical Knowledge Discovery over Ontology Matching Results. IRMLeS 2009 at ESWC-2009.
</p> -->
<!--
<p>
[1] Euzenat J. et al.: <a href="http://ceur-ws.org/Vol-814/oaei11_paper0.pdf">Results of the Ontology Alignment Evaluation Initiative 2011</a>. 
</p> -->
<!--
<p>
[5] Fleischhacker D., Stuckenschmidt H.: Implementing semantic
precision and recall. accepted for poster session at OM-2009 at ISWC
2009.
</p>

<p>
[6] Meilicke C., Stuckenschmidt H.. An Efficient Method for Computing a Local Optimal Alignment Diagnosis. Technical Report, University Mannheim, 2009.
</p> -->
<!--
<p>
[7] Euzenat J. et al.: <a http://www.dit.unitn.it/~p2p/OM-2010/oaei10_paper0.pdf">Results of the Ontology Alignment Evaluation Initiative 2010</a>. 
</p>
-->
<div class="address">
<div class="footer"><a href="http://oaei.ontologymatching.org/2017/conference/index.html">http://oaei.ontologymatching.org/2017/conference/index.html</a></div>
<!--$Id$-->
</div>
</body></html>