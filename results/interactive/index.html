
<html>
<head>
<title>Ontology Alignment Evaluation Initiative::Interactive</title>
<link rel="stylesheet" type="text/css" href="http://oaei.ontologymatching.org/2016/style.css" />
<style type="text/css">
#Kopfbereich { position:absolute; top:20px; left:30px; }
#table { position:static; top:100px; left:30px; }
body { background-color:#FFFFFF; }
th,td { padding:1px; }
td { font-family:Courier New,courier; text-align:center; border:solid 1px #808080 }
table { border:solid 1px }
</style>
<script type="text/javascript">

<!-- array containg all data -->
<!--corresponds to error rate 0.0-->
var anatomy0 = new Array(
  "ALIN","1074","1211.0","0.993","0.794","0.882","0.454","0.985","0.339","0.504","0.0","0.993","0.794","0.882","939","1472","689.0","783.0","0.0","0.0","1.0","1.0",
  "AML","45","1484.0","0.968","0.948","0.958","0.862","0.95","0.936","0.943","0.832","0.968","0.948","0.958","241","240","51.0","189.0","0.0","0.0","1.0","1.0",
  "LogMap","23","1306.0","0.982","0.846","0.909","0.595","0.911","0.846","0.877","0.593","0.982","0.846","0.909","388","1164","287.0","877.0","0.0","0.0","1.0","1.0",
  "XMap","43","1415.0","0.927","0.865","0.895","0.644","0.926","0.863","0.893","0.639","0.927","0.865","0.895","35","35","5.0","30.0","0.0","0.0","1.0","1.0"
);

<!--corresponds to error rate 0.1-->
var anatomy1 = new Array(
  "ALIN","1000","1201.5","0.94","0.745","0.831","0.403","0.985","0.339","0.504","0.0","0.993","0.79","0.88","905","1352","615.0","603.7","64.5","69.5","0.905","0.897",
  "AML","45","1499.6","0.956","0.946","0.95","0.856","0.95","0.936","0.943","0.832","0.969","0.949","0.959","266","264","50.4","189.5","19.5","5.5","0.73","0.972",
  "LogMap","23","1307.0","0.962","0.83","0.891","0.564","0.911","0.846","0.877","0.593","0.966","0.803","0.877","388","1164","257.6","790.1","86.9","29.4","0.748","0.964",
  "XMap","44","1415.1","0.927","0.865","0.895","0.644","0.926","0.863","0.893","0.639","0.927","0.863","0.894","35","35","4.0","27.1","3.0","1.0","0.602","0.964"
);

<!--corresponds to error rate 0.2-->
var anatomy2 = new Array(
  "ALIN","994","1190.6","0.895","0.703","0.787","0.358","0.985","0.339","0.504","0.0","0.993","0.788","0.879","891","1311","551.2","511.6","117.4","131.4","0.824","0.796",
  "AML","46","1520.0","0.939","0.942","0.94","0.849","0.95","0.936","0.943","0.832","0.969","0.951","0.96","283","280","48.4","173.5","46.0","12.9","0.513","0.93",
  "LogMap","23","1320.9","0.944","0.823","0.88","0.552","0.911","0.846","0.877","0.593","0.945","0.762","0.843","388","1164","231.8","699.3","177.7","55.2","0.566","0.927",
  "XMap","44","1415.2","0.927","0.865","0.895","0.644","0.926","0.863","0.893","0.639","0.927","0.862","0.893","35","35","4.1","24.1","6.4","0.9","0.422","0.964",
);

<!--corresponds to error rate 0.3-->
var anatomy3 = new Array(
  "ALIN","980","1162.8","0.846","0.649","0.735","0.301","0.985","0.339","0.504","0.0","0.993","0.781","0.874","882","1266","469.9","417.9","170.9","207.6","0.734","0.668",
  "AML","45","1544.5","0.922","0.939","0.931","0.843","0.95","0.936","0.943","0.832","0.97","0.952","0.961","310","308","42.4","171.8","75.6","18.6","0.359","0.902",
  "LogMap","23","1332.9","0.931","0.82","0.872","0.544","0.911","0.846","0.877","0.593","0.92","0.722","0.809","388","1164","203.2","609.1","267.9","83.8","0.431","0.879",
  "XMap","44","1415.2","0.927","0.865","0.895","0.644","0.926","0.863","0.893","0.639","0.927","0.861","0.893","35","35","3.3","21.6","8.4","1.7","0.278","0.93"
);

var conference0 = new Array(
  "ALIN","35","0.957","0.731","0.829","0.892","0.272","0.417","0.957","0.731","0.829","329","571","140.0","431.5","0.0","0.0","1.0","1.0",
  "AML","30","0.912","0.711","0.799","0.841","0.659","0.739","0.912","0.711","0.799","271","270","47.0","223.0","0.0","0.0","1.0","1.0",
  "LogMap","35","0.886","0.61","0.723","0.818","0.59","0.686","0.886","0.61","0.723","82","246","49.0","197.0","0.0","0.0","1.0","1.0",
  "XMap","21","0.837","0.57","0.678","0.837","0.57","0.678","0.837","0.57","0.678","4","4","0.0","4.0","0.0","0.0","0.0","1.0"
);

var conference1 = new Array(
  "ALIN","35","0.804","0.669","0.73","0.892","0.272","0.417","0.961","0.737","0.834","321","549","121.0","374.6","40.0","13.9","0.752","0.966",
  "AML","30","0.841","0.701","0.765","0.841","0.659","0.739","0.923","0.732","0.816","282","275","50.8","198.6","21.0","5.4","0.704","0.975",
  "LogMap","35","0.851","0.598","0.702","0.818","0.59","0.686","0.855","0.573","0.686","82","246","44.2","177.4","19.6","4.8","0.698","0.978",
  "XMap","21","0.837","0.57","0.678","0.837","0.57","0.678","0.837","0.57","0.678","4","4","0.0","3.9","0.1","0.0","0.0","1.0",
);

var conference2 = new Array(
  "ALIN","36","0.669","0.622","0.645","0.892","0.272","0.417","0.965","0.751","0.845","313","534","106.8","319.3","84.2","24.4","0.558","0.93",
  "AML","30","0.768","0.672","0.717","0.841","0.659","0.739","0.925","0.745","0.825","292","279","49.5","172.7","42.0","15.6","0.538","0.92",
  "LogMap","35","0.821","0.585","0.684","0.818","0.59","0.686","0.829","0.542","0.656","82","246","38.3","159.1","37.9","10.7","0.507","0.941",
  "XMap","21","0.837","0.57","0.678","0.837","0.57","0.678","0.837","0.569","0.677","4","4","0.0","3.5","0.5","0.0","0.0","1.0",
);

var conference3 = new Array(
  "ALIN","35","0.577","0.56","0.568","0.892","0.272","0.417","0.966","0.752","0.845","302","517","87.7","274.4","115.2","39.9","0.431","0.875",
  "AML","20","0.713","0.651","0.68","0.841","0.659","0.739","0.929","0.751","0.83","291","274","49.4","143.9","60.6","20.7","0.45","0.877",
  "LogMap","35","0.795","0.581","0.671","0.818","0.59","0.686","0.807","0.518","0.631","82","246","33.2","138.1","58.9","15.8","0.363","0.902",
  "XMap","22","0.837","0.57","0.678","0.837","0.57","0.678","0.837","0.569","0.678","4","4","0.0","3.5","0.5","0.0","0.0","1.0"
);

var largeBio0 = new Array(
"AML","0.943","0.905","0.771","0.754","0.845","0.819","0.943","0.771","0.845","10217","10217","5126","5091","0","0","2877",
"LogMap","0.971","0.903","0.725","0.714","0.826","0.794","0.971","0.725","0.826","27436","27436","17050","10386","0","0","3803",
"ServOMBI*","1.000","0.965","0.737","0.735","0.847","0.833","1.000","0.737","0.847","21416","9424","8685","739","0","0","726"
);

var largeBio1 = new Array(
"AML","0.931","0.905","0.761","0.754","0.835","0.819","0.944","0.769","0.845","10217","10217","4624","4658","485","450","2913",
"LogMap","0.939","0.903","0.701","0.714","0.798","0.794","0.944","0.704","0.803","28890","28890","15753","10659","1181","1297","3963",
"ServOMBI*","0.995","0.965","0.666","0.735","0.796","0.833","1.000","0.716","0.832","22920","9502","8063","726","85","628","695"
);

var largeBio2 = new Array(
"AML","0.917","0.905","0.753","0.754","0.824","0.819","0.944","0.769","0.845","10217","10217","4196","4081","1049","891","2930",
"LogMap","0.915","0.903","0.684","0.714","0.778","0.794","0.907","0.676","0.771","30426","30426","14286","10707","2669","2764","3912",
"ServOMBI*","0.987","0.965","0.592","0.735","0.739","0.833","1.000","0.688","0.813","23968","9541","7431","661","192","1257","713"
);

var largeBio3 = new Array(
"AML","0.906","0.905","0.745","0.754","0.815","0.819","0.943","0.771","0.845","10217","10217","3737","3637","1537","1306","2959",
"LogMap","0.898","0.903","0.681","0.714","0.769","0.794","0.868","0.649","0.739","31504","31504","13035","10147","4307","4015","3874",
"ServOMBI*","0.983","0.965","0.523","0.735","0.681","0.833","1.000","0.660","0.792","25580","9600","6818","652","256","1874","618"
);


<!-- array in which the sorted data is located -->
<!--var sort_data = new Array(data.length);-->

<!-- define the headings, headings[x][0] are the names of the upper headings and all others for subheadings -->
<!-- below a certain upper heading -->

var anatomyHeadings = new Array("Tool","Run Time (sec)","Size","Precision","Recall","F-measure","Recall+","Precision Non Inter","Recall Non Inter","F-measure Non Inter","Recall+ Non Inter","Precision Oracle","Recall Oracle","F-measure Oracle","Total Requests","Distinct Mappings","True Positives","True Negatives","False Positives","False Negative","Precision","Negative Precision");

<!-- var conferenceHeadings = new Array("Tool","Precision","Precision non","Recall","Recall non","F-measure","F-measure non","Precision Oracle","Recall Oracle","F-measure Oracle","Total Requests","Distinct Mappings","True Positives","True Negatives","False Positives","False Negatives","Run Time");
 -->
var conferenceHeadings = new Array("Tool","Run Time (sec)","Precision","Recall","F-measure","Precision Non Inter","Recall Non Inter","F-measure Non Inter","Precision Oracle","Recall Oracle","F-measure Oracle","Total Requests","Distinct Mappings","True Positives","True Negatives","False Positives","False Negative","Precision","Negative Precision");

var largeBioHeadings = new Array("Tool","Precision","Precision non","Recall","Recall non","F-measure","F-measure non","Precision Oracle","Recall Oracle","F-measure Oracle","Total Requests","Distinct Mappings","True Positives","True Negatives","False Positives","False Negatives","Run Time");

<!-- determine the number of columns -->
<!--var columns = headings.length; -->
<!--var rows = data.length/columns; -->

<!-- different heading formats according to the number of subheadings -->
var headingColumnsFormatting = new Array(
 "colspan=\"1\" valign=\"top\" style=\"text-align:center; border:solid 1px #808080\"",
 "colspan=\"6\" valign=\"top\" style=\"text-align:center; border:solid 1px #808080\"",
 "colspan=\"1\" valign=\"top\" style=\"text-align:center; border:solid 1px #808080\""

);

<!-- set the style of all other columns-->
var columnsFormatting = new Array(
 "width=\"75\" style=\"font-family:Courier New,courier; text-align:center; border:solid 1px #808080\"",
 "width=\"75\" bgcolor=\"#00FFFF\" style=\"font-family:Courier New,courier; text-align:center; border:solid 1px #808080\""
);

<!-- for every column the sort type is set, e.g. alphabetical if the column should be sorted alphabetically -->
var conferenceSortTypes = new Array(
 "alphabetical","numerical","numerical","numerical","numerical","numerical","numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical","numerical", "numerical", "numerical", "numerical", "numerical"
);

var largeBioSortTypes = new Array(
 "alphabetical","numerical","numerical","numerical","numerical","numerical","numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical","numerical", "numerical", "numerical"
);

var anatomySortTypes = new Array(
 "alphabetical","numerical","numerical","numerical","numerical","numerical","numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical", "numerical"
);

<!-- table format -->
var tableFormatting = "border=\"1\" style=\"border:solid 1px #808080\" cellspacing=\"0\"";

<!-- load the arrow pictures -->
var ArrowUp = "<img src=\"arrowUp.jpg\" width=\"14\" height=\"12\" border=\"0\" alt=\"\">";
var ArrowDown = "<img src=\"arrowDown.jpg\" width=\"14\" height=\"12\" border=\"0\" alt=\"\">";
var ArrowUpSorted = "<img src=\"arrowUpSorted.jpg\" width=\"14\" height=\"12\" border=\"0\" alt=\"\">";
var ArrowDownSorted = "<img src=\"arrowDownSorted.jpg\" width=\"14\" height=\"12\" border=\"0\" alt=\"\">";

<!--var sortedLine = "";-->

function stringifyArray(Array){
	var arrayAsString = "[";
	for(var i=0;i < Array.length;i++)
	{
		arrayAsString+="'" + Array[i] + "'";
		if(!((i+1) == Array.length))
		{
		arrayAsString+=","
		}
	}

	arrayAsString+= "]";
	return arrayAsString;
}


<!-- function to sort a certain line in a certain direction. Number presents the lien number. -->
function createSortedLine(number,headings,direction,array,divElementIDToWrite,columnSortTypes) {
 var sortedLine = "<tr>";
 var columns = headings.length;
 var rows = array.length/columns;
 for(var j = 0; j < columns; ++j) {
  sortedLine += "<th " + columnsFormatting[0] + ">";

  <!-- sort ascending and print the right arrow in red-->
  if(direction == "ascending" && j == number) {
   sortedLine += ArrowUpSorted + " ";
   sortedLine += "<a href=\"javascript:sortColumns(" + j + ","+ columns+","+rows+",'"+columnSortTypes[j] + "','descending'," + stringifyArray(array) + "," + stringifyArray(headings) + ",'" + divElementIDToWrite + "'," + stringifyArray(columnSortTypes) + ")\">" + ArrowDown + "</a>";
  }
  <!-- sort descending and print the right arrow in red--->
  else if(direction == "descending" && j == number) {
   sortedLine += "<a href=\"javascript:sortColumns(" + j + ","+ columns+","+rows+",'"+ columnSortTypes[j] + "','ascending'," + stringifyArray(array) + "," + stringifyArray(headings) + ",'" + divElementIDToWrite + "'," + stringifyArray(columnSortTypes) + ")\">" + ArrowUp + "</a>";
   sortedLine += " " + ArrowDownSorted;
  }
  <!-- print all other columns -->
  else {
   sortedLine += "<a href=\"javascript:sortColumns(" + j + ","+ columns+","+rows+",'"+  columnSortTypes[j] + "','ascending'," + stringifyArray(array) + "," +  stringifyArray(headings) + ",'" + divElementIDToWrite + "'," + stringifyArray(columnSortTypes) + ")\">" + ArrowUp + "</a> ";
   sortedLine += "<a href=\"javascript:sortColumns(" + j + ","+ columns+","+rows+",'"+  columnSortTypes[j] + "','descending'," + stringifyArray(array) + "," +  stringifyArray(headings) + ",'" + divElementIDToWrite + "'," + stringifyArray(columnSortTypes) + ")\">" + ArrowDown + "</a><\/td>";
  }
 sortedLine += "<\/th>";
 }
 sortedLine += "<tr>";
 return sortedLine;
}

<!-- function to really sort the columns according to a certain line number, type and direction -->
function sortColumns(number,columns,rows,type,direction,array,headings,divElementIDToWrite,columnSortTypes) {
 var sort_data = new Array();
 var columnsData = new Array();
 var dataToCompare = new Array();
 var index = new Array();
 <!-- get the according data -->
 for(var i = 0; i < rows; ++i)
  columnsData[i] = dataToCompare[i] = array[i * columns + number];
 if(type == "alphabetical") columnsData.sort();
 if(type == "numerical") columnsData.sort(Numsort);
 if(type == "numericalInverse") columnsData.sort(NumsortInverse);
 if(type== "star") columnsData.sort(starSort);
 if(direction == "descending") columnsData.reverse();
 <!--- determine and set the according index -->
 var alreadySet = false;
 for(i = 0; i < rows; ++i)
  for(var j = 0; j < rows; ++j)
   if(columnsData[i] == dataToCompare[j]) {
   <!-- check whether the current j is already set as index to avoid problems with same numbers -->
	for(var k=0; k<index.length; k++) {
	 if(index[k]==j) alreadySet=true;
	 }
     if(alreadySet==false) index[i] = j;
	 else alreadySet=false;
   }

 <!-- create the sorted line and write it -->
 for(i = 0; i < rows; ++i)
  for(j = 0; j < columns; ++j)
   sort_data[i * columns + j] = array[index[i] * columns + j];
 <!--createSortedLine(number,headings,direction,array,divElementIDToWrite);-->
 writeTable(sort_data,headings,number,direction,divElementIDToWrite,columnSortTypes);
}

<!-- write the whole table with specified columns -->
function writeTable(array,headings,number,direction,divElementIDToWrite,columnSortTypes) {
 columns = headings.length;
 rows=array.length/columns;
 var content = "";
 content += "<table " + tableFormatting + ">";
 content += "<thead><tr>";
 for(var j = 0; j < headings.length; ++j)
  content += "<th " + headingColumnsFormatting[0] + "bgcolor=\"CCCCCC\">" + headings[j] + "<\/th>";
  content += "<\/tr>";
<!--  content += "<tr>"; -->
  <!--for(var j = 0; j < headings.length; ++j) {-->

<!--	for(var k = 1; k < headings[j].length; ++k)-->
<!--		content += "<th " + headingColumnsFormatting[5] + ">" + headings[j][k] + "<\/th>"; -->

<!--  } -->
<!--  content += "<\/tr>";	-->
 content += createSortedLine(number,headings,direction,array,divElementIDToWrite,columnSortTypes);
 content += "<\/thead>";
 content += "<tfoot><\/tfoot>";
 content += "<tbody>";
 for(var i = 0; i < rows; ++i) {
	  if(i%2 == 0) {
          content += "<tr bgcolor=\"FFFFDD\">";
      }
	  else {
          content += "<tr bgcolor=\"FFDDFF\">";
      }
	  if(array[i * columns] == array[14]) {
          content += "<tr bgcolor=\"DCDCDC\">";
      }
	  if(array[i * columns] == array[28]) {
          content += "<tr bgcolor=\"DCDCDC\">";
      }
  for(var j = 0; j < columns; ++j)
   content += "<td " + columnsFormatting[0] + ">" + array[i * columns + j] + "<\/td>";
  content += "<\/tr>";
 }
 content += "<\/tbody>";
 content += "<\/table>";
 var element = document.getElementById(divElementIDToWrite);
 if(element!=null)
 {
  document.getElementById(divElementIDToWrite).innerHTML = content;
 }
 else if(document.all)
 {
  document.all.table.innerHTML = content;
  }
 else if(document.layers) {
  document.table.document.open();
  document.table.document.write(content);
  document.table.document.close();
 }
}
<!-- sorting for numerical values-->
function Numsort(a,b){
    if(a=="-") {
        return -1;
    }
    if(b=="-") {
        return 1;
    }
    return a-b;
}

function starSort(a,b){
	if(a.indexOf("*")!=-1) {
		a=a.replace("*","");
	}
	if(a.indexOf("*")!=-1) {
		a=a.replace("*","");
	}
	if(b.indexOf("*")!=-1) {
		b=b.replace("*","");
	}
	if(b.indexOf("*")!=-1) {
		b=b.replace("*","");
	}
	var result = Numsort(a,b);
	return result;
}

<!-- sorting for inverse numerical values-->
function NumsortInverse(a,b){
    if(a=="-") {
        return -1;
    }
    if(b=="-") {
        return 1;
    }
	if(a=='X') {
		return -1;
	}
	if(b=='X') {
		return 1;
	}
	if(a=='T') {
		return -1;
	}
	if(b=='T') {
		return 1;
	}
    return b-a;
}
</script>
<title>table</title>
</head>
<!--<body onLoad="createSortedLine(-1,'',data); writeTable(data);">-->
 <body onLoad="writeTable(anatomy0,anatomyHeadings,-1,'','anatomy0',anatomySortTypes);writeTable(anatomy1,anatomyHeadings,-1,'','anatomy1',anatomySortTypes);writeTable(anatomy2,anatomyHeadings,-1,'','anatomy2',anatomySortTypes);writeTable(anatomy3,anatomyHeadings,-1,'','anatomy3',anatomySortTypes);writeTable(conference0,conferenceHeadings,-1,'','conference0',conferenceSortTypes);writeTable(conference1,conferenceHeadings,-1,'','conference1',conferenceSortTypes);writeTable(conference2,conferenceHeadings,-1,'','conference2',conferenceSortTypes);writeTable(conference3,conferenceHeadings,-1,'','conference3',conferenceSortTypes);writeTable(largeBio0,largeBioHeadings,-1,'','largeBio0',largeBioSortTypes);writeTable(largeBio1,largeBioHeadings,-1,'','largeBio1',largeBioSortTypes);writeTable(largeBio2,largeBioHeadings,-1,'','largeBio2',largeBioSortTypes);writeTable(largeBio3,largeBioHeadings,-1,'','largeBio3',largeBioSortTypes);">
<br/>
<div class="header">
<a style="color: grey; line-height: 5mm;" href="http://oaei.ontologymatching.org/2017/">Ontology
  Alignment Evaluation Initiative - OAEI-2017 Campaign</a>
  <a href="http://oaei.ontologymatching.org/">
  <br/>
  <br/>
  <br/>
  <!--
  <img  width="150px" src="http://oaei.ontologymatching.org/oaeismall.jpg" style="clear:left;float:left; margin-left: 1pt; border-style:none;"/></a>
  <img width="150px" src="http://oaei.ontologymatching.org/oaeismall.jpg" style="float:right; margin-left: 5pt; border-style:none;"/></a>
  -->
  <img  width="150px" src="../../../oaeismall.jpg" style="clear:left;float:left; margin-left: 1pt; border-style:none;"/></a>
  <img width="150px" src="../../../oaeismall.jpg" style="float:right; margin-left: 5pt; border-style:none;"/></a>
 <a href="http://www.seals-project.eu/">
<img  width="200px" src="Seals-logo-5.png" style="clear:right;float:right; margin-left: 5pt; border-style:none;"/></a>
</div>
<br/>
<center>

<h1>Results for OAEI 2017 - Interactive Track</h1>
</center>
<br/>
<br/>

<!-- UNCOMMENT for the final version of the page <p><i>The following content is (mainly) based on the final version of the interactive section in the OAEI results paper.<br/> If you notice any kind of error (wrong numbers, incorrect information on a matching system) do not hesitate to contact us.</i></p> -->

<br/>
<h2>Description</h2>

<p>The growth of the ontology alignment area in the past ten years has led to the development of many ontology alignment tools.
After several years of experience in the OAEI, we observed that the results only slightly improved in terms of the
alignment quality (precision/recall resp. F-measure). Based on this insight, it is clear that fully automatic ontology matching
approaches slowly reach an upper bound of the alignment quality they can achieve. A work by <a href="#references">(Jimenez-Ruiz et al., 2012)</a> has shown
that simulating user interactions with 30% error rate during the alignment process has led to the same results as non-interactive matching.
Thus, in addition to the validation of the automatically generated alignments by domain experts, we believe that there is
further room for improving the quality of the generated alignments by incorporating user interaction. User involvement during
the matching process has been identified as one of the challenges in front of the ontology alignment community by <a href="#references">(Shvaiko et al., 2013)</a>
and user interaction with a system is an integral part of it.</p>

<p>At the same time with the tendency of increasing ontology sizes, the alignment problem also grows.
It is not feasible for a user to, for instance validate all candidate mappings generated by a system, i.e.,
tool developers should aim at reducing unnecessary user interventions. All required efforts of the human have
to be taken into account and it has to be in an appropriate proportion to the result.
Thus, beside the quality of the alignment, other measures like the number of interactions are interesting and meaningful
to decide which matching system is best suitable for a certain matching task. By now, all OAEI tracks focus on
fully automatic matching and semi-automatic matching is not evaluated although such systems already exist, e.g.,
overview in <a href="#references">(Ivanova et al., 2015)</a>. As long as the evaluation of such systems is not driven forward,
it is hardly possible to systematically compare the quality of interactive matching approaches.</p>

<br/>
<h2>Datasets</h2>
<p>
In this 5th edition of the Interactive track we use four OAEI datasets, namely Conference, Anatomy, Phenotype and Large Biomedical Ontologies (LargeBio).
The Conference dataset covers 16 ontologies describing the domain of conference organization.
We only use the test cases for which an alignment is publicly available (altogether 21 alignments/tasks).

The Anatomy dataset includes two ontologies (1 task), the <a href="http://www.informatics.jax.org/searches/AMA_form.shtml">Adult Mouse Anatomy</a> (AMA) ontology and a part of the <a href="http://ncit.nci.nih.gov/">National Cancer Institute Thesaurus</a> (NCI) describing the human anatomy.
The Phenotype dataset has four ontologies which are the <a href="http://bioportal.bioontology.org/ontologies/HP">Human Phenotype Ontology (HPO)</a>,
the <a href="http://bioportal.bioontology.org/ontologies/MP">Mammalian Phenotype Ontology (MP)</a>,
the <a href="http://bioportal.bioontology.org/ontologies/DOID">Human Disease Ontology (DOID)</a>, and
the <a href="http://bioportal.bioontology.org/ontologies/ORDO">Orphanet and Rare Diseases Ontology (ORDO)</a>.

Finally, the LargeBio consists of 6 tasks with different sizes ranging from tens to hundreds of thousands classes and aims at finding alignments between the <a href="http://sig.biostr.washington.edu/projects/fm/">Foundational Model of Anatomy</a> (FMA),
<a href="http://www.ihtsdo.org/index.php?id=545">SNOMED CT</a>,
and the <a href="http://ncit.nci.nih.gov/">National Cancer Institute Thesaurus</a> (NCI).
</p>
<p>
The quality of the generated alignments in the Conference, Anatomy and LargeBio tracks has been constantly increasing but in most cases only by a small amount (by a few percent).
For example, in the Conference track in 2013, the best system according to F-measure was YAM++ with 70% <a href="#references">(Cuenca Grau et al., 2013)</a>. On the other hand, while
the best result according to F-measure for the Anatomy track was achieved last year by AML with 94% there has been very little improvement over previous few campaigns
(only few percent). This shows that there is room for improvement, which could be filled by interactive means.
</p>


<h2>Experimental Setting</h2>
<p>
The interactive matching track was organized at OAEI 2017 for the fifth time.
The goal of this evaluation is to simulate interactive matching (see <a href="#references">(Dragisic et al., 2016)</a> and <a href="#references">(Paulheim et al., 2013)</a>),
where a human expert is involved to validate mappings found by the matching system.
In the evaluation, we look at how interacting with the user improves the matching results.
The SEALS client was modified to allow interactive matchers to ask an oracle.
The interactive matcher can present a correspondence to the oracle, which then tells the system whether the correspondence is right or wrong.
Same as last year we have extended this functionality - a matcher can present simultaneously several correspondences to the oracle.
Similarly to the last two years, this year, in addition to emulating the perfect user, we also consider domain experts with variable error rates which reflects a more realistic scenario where a (simulated) user does not necessarily provide a correct answer. We experiment with three different error rates, 0.1, 0.2 and 0.3. The errors were randomly introduced into the reference alignment with given rates.
</p>
<p>
The evaluations of the Conference and Anatomy datasets were run on a server with 3.46 GHz (6 cores) and 8GB RAM allocated to the matching systems. Each system was run ten times and the final result of a system for each error rate represents the average of these runs. This is the same configuration which was used in the non-interactive version of the Anatomy track and runtimes in the interactive version of this track are therefore comparable.
For the <a href="http://oaei.ontologymatching.org/2014/conference/eval.html">Conference dataset<a/> with the ra1 alignment, we considered macro-average of precision and recall of different ontology pairs, while the number of interactions represent the total number of interactions in all tasks. Finally, the ten runs are averaged.
The evaluations of the Phenotype and
the LargeBio datasets (each system was run one time) were run on a Ubuntu Laptop with an Intel Core i7-4600U CPU @ 2.10GHz x 4 and allocating 15Gb of RAM.
</p>
<!--
<h2>Systems</h2>
<p>
Four systems participated in the Interactive matching track: <i>ALIN</i>, <i>AML</i>, <i>LogMap</i> and <i>XMap</i>.
All systems participating in the Interactive track support both interactive and non-interactive matching.
This allows us to analyze how much benefit the interaction brings for the individual system.
</p>

<p>
The different systems involve the user at different points of the execution and employ the user input in different ways.
Therefore, we describe how the interaction is done by each system.
<i>AML</i> starts interacting with the user during the Selection and Repairing phases (for the LargeBio task only non-interactive repair is employed) at the end of the matching process.
The user input is employed to filter the mappings included in the final alignment and AML does not generate new mappings or adjust matching parameters based on it.
AML avoids asking the same question more than once by keeping track of already asked questions and uses a query limit and other strategies to stop asking the user and revert to the non-interactive mode.
<i>ALIN</i> generates an initial set of candidate mappings between classes based on 6 string similarity metrics and a stable marriage algorithm.
It uses a stable marriage algorithm with Incomplete Preference Lists which means that neither the sizes of the two sets nor the preference lists need to be with the same lengths.
The stable marriage algorithm is run for each similarity metric with a preference list of size one.
A pair of classes is considered a candidate mapping if its preference list only contains the other class in the pair for one similarity metric.
If a candidate mapping has the maximum similarity value, i.e., 1, for all six metrics than it is directly added to the final alignment.
The rest are sorted by the sum of the metrics and are presented one by one to the user.
If a full entity name in a candidate mapping is the same as another entity name in another candidate mapping they are shown to the expert simultaneously (up to three at the same time).
ALIN does not ask the same question more than once. When the user accepts a candidate mapping, it is moved to the final alignment.
The data and object correspondences related to the accepted mapping are then added to the candidate mappings list and all candidate mappings that are part of alignment anti-patterns with the approved mapping are removed from the list with the candidates.
The process continues until there are no more candidate mappings left.
<i>LogMap</i> generates candidate mappings first and then employs different techniques (lexical, structural and reasoning-based) to discard some of them during the Assessment phase.
During this phase in the interactive mode it interacts with the user and presents to him/her these mappings which are not clear-cut cases.
In this year, <i>LogMap</i> makes use of the functionality of the seal client where one can ask to the oracle up to 3 related mappings including ">", "<" and "=" relationships in one request.
<i>XMap</i> uses various similarity measures to generate candidate mappings.
It applies two thresholds to filter the candidate mappings - one for the mappings that are directly added to the final alignment and another for those that are presented to the user for validation.
The latter threshold is selected to be high in order to minimize the number of requests and the rejected candidate mappings from the oracle; the requests are mainly about incorrect mappings.
The mappings accepted by the user are moved to the final alignment.</p>

-->
<h2>Results</h2>

The results are presented for each dataset separately. Columns "Precision Oracle", "Recall Oracle" and "F-measure Oracle" across all tables for all datasets that follow below
contain the evaluation results "according to the Oracle", meaning against the Oracle's alignment (i.e., the reference alignment as modified by the randomly introduced errors). They represent the
precision, recall and F-measure of the matching tool as if the oracle answers were all correct and show the impact of the oracle's errors on the tool.
The "Total Requests" column represents the number of distinct user interactions with the tool,
where each interaction can contain one or more mappings that could be analyzed simultaneously. The mappings that are not conflicting are counted individually;
and if more than three mappings are given, they are all counted independently, regardless of whether they are conflicting. This value is given in the "Distinct Mappings" column.
The next four columns represent the true positives, true negatives, false positives and false negatives out of all distinct mappings requests. The "Precision" and "Negative Precision" columns are the
precision and negative precision of the oracle itself, i.e., respectively the fraction of positive and negative answers given by the oracle that are correct.
The "Precision" value affects the true positives and false positives, and the "Negative Precision" value affects the true negatives and false negatives.

<table>
  <tr>
    <th>Measure</th>
    <th>Description</th>
	<th>Meaning for the evaluation</th>
  </tr>
  <tr>
    <td><b>Precision, Recall, F-measure</b></td>
    <td>Measure the tool's performance against the fixed reference alignment.</td>  <!-- , letting us know in absolute terms how the tools performed in the matching problem -->
    <td>Show in absolute terms how the tool performed in the matching problem.</td>
  </tr>
  <tr>
    <td><b>Precision Oracle, Recall Oracle and F-measure Oracle</b></td>
    <td>Measure the tool's performance against the reference as modified by the oracle's errors.</td> <!-- , letting us know how the tool is impacted by the errors (only directly vs also indirectly or in other words linearly vs supra-linearly). -->
    <td>Show how the tool is impacted by the errors.</td>
  </tr>
  <tr>
    <td><b>Precision and Negative Precision</b></td>
    <td>Measure the performance of the oracle itself.</td> <!-- , letting us know what type of errors the oracle made, and thus explain the performance of the tool when faced with these errors -->
    <td>Show what type of errors the oracle made, and thus explain the performance of the tool when faced with these errors.</td>
  </tr>
</table>
<noscript>
<p>Java-Script need to be enabled!</p>
</noscript>

 <br/>
 <h3>Anatomy Track</h3>

  <p>
  The four tables below present the results for the Anatomy dataset with four different error rates.
  The first two columns in each of the tables present the run times (in seconds) and the size of the alignment.
  The next eight columns present the precision, recall, F-measure and recall+ obtained from the interactive and non-interactive tracks for the Anatomy dataset.
  The measure recall+ indicates the amount of detected non-trivial equivalence correspondences.
  To calculate it the trivial correspondences (those with the same normalized label including those in the oboInOwlnamespace as well) have been removed as well as correspondences expressing relations different from equivalence.
  The meaning of the other columns is described above at the beginning of the Results section.
<!--  The next three columns contain the evaluation results "according to the Oracle", meaning against the Oracle's alignment
  (i.e., the reference alignment as modified by the randomly introduced errors).-->
<!--  The next column represents the number of distinct user interactions (column "Total Requests") where each interaction can conatain one or more mappings that could be analyzed simultaneously.
  The mappings that are not conflicting are counted individually; and if more than three mappings are given, they are all counted independently, regardless of whether they are conflicting.
  This value is given in the "Distinct Mappings" column.
  The next four columns represent the true positive, true negative, false positive and false negative out of all distinct request.
  The last two columns are the the precision and negative precision of the oracle itself, i.e., respectively the fraction of positive and negative answers given by the oracle that are correct.-->
  Fig. 1 shows the time intervals between the questions to the user/oracle for the different systems and error rates for the ten runs (the runs are depicted with different colors).
  </p>


 <h4>Error Rate 0.0</h4>
 <div id="anatomy0"></div>
 <h4>Error Rate 0.1</h4>
 <div id="anatomy1"></div>
 <h4>Error Rate 0.2</h4>
 <div id="anatomy2"></div>
 <h4>Error Rate 0.3</h4>
 <div id="anatomy3"></div>


 <p>
 We first compare the performance of the four systems with an <b>all-knowing oracle</b> (0.0 error rate), in terms of precision, recall, F-measure and recall+, to the results obtained in the non-interactive Anatomy track ("Precision", "Recall", "F-measure", "Recall+", "Precision Non Inter", "Recall Non Inter", "F-measure Non Inter" and	"Recall+ Non Inter").
 The effect of introducing interactions with the oracle/user is mostly pronounced for the ALIN system and especially for its recall measure - ALIN's recall increases more than 2 times.
 On the opposite side is XMap - it benefits the least from the interaction with the oracle.
 XMap differs with less than 0.2 percentage point in terms of Precision, Recall, F-measure and 0.5 percentage point regarding Recall+ from the non-interactive runs.
 At the same time AML's recall increases with less than 2 percentage points and LogMap's recall does not change.
 User interactions affect differently the precision measure - it increases for all systems and it is most advantageous for LogMap which precision increases with around 7.1 percentage points.
 Consequently, the ALIN's F-measure increases the most (from 0.504 to 0.882), the F-measure for LogMap and AML slightly changes.
 AML has the highest  F-measure for this error rate as well as for the rest of the tested error rates.
 It is worth noting that ALIN detects only trivial correspondences in the non-interactive track ("Recall+ Non Inter" = 0), introducing user interactions led to detecting some non-trivial correspondences ("Recall+" = 0.454).
 Thus it seems that ALIN is relying only on user interaction to generate non-trivial mappings.
 The recall+ slightly improves for LogMap and XMap and with around 3 percentage points for AML.
 In terms of the alignment size and the number of total requests,
 AML generates the largest alignment, XMap uses the least number of requests to the oracle while ALIN generates the smallest alignment with almost 4 times more user interactions and about 6 times more distinct mappings then AML.
 <p>
 </p>
 With the introduction of an erroneous oracle/user and <b>moving towards higher error rates</b> the systems' performance starts to slightly deteriorate in comparison to the all-knowing oracle.
 Very noticeably XMap's performance does not change at all with the increasing error rates.
 For the other three we can observe that the changes in the error rates influence the systems differently in comparison to the non-interactive results.
 The AML performance with an all-knowing oracle is better on all measures. To compare the interactive results with non-interactive results, the F-measure drops in the 0.2 and 0.3 cases, while the recall stays higher than the non-interactive results for all error rates.

 LogMap behaves similarly - the F-measure in the 0.3 case drops below the non-interactive results, while the precision stays higher in all error rates.

 ALIN's F-measure and recall are higher for all error rates in comparison to its non-interactive results, but its precision drops already in the 0.1 case.

 Overall the F-measure drops with about 15 percentage points for ALIN from error rate 0.0 to 0.3 and with around 3.7 and 2.7 percentage points respectively for LogMap and AML.
 One observation from the recall+ in interactive and non-interactive tracks, ALIN, AML and XMap have higher number in interactive track for all error rates which means they generated more non-trival mappings.
 The recall+ of LogMap in interactive track is higher than that in non-interactive only for all-knowing oracle.
 Now let us examine how the precision and recall are affected when moving from error rate 0.0 to 0.3 in connection to the values in the last two columns - <b>"Precision" and "Negative Precision"</b>.
 The "Precision" value drops more than the value for "Negative Precision" for both AML and LogMap, with a very noticeable difference for AML.
 Taking into account that the "Precision" impacts precision and "Negative Precision" impacts recall we would expect that the precision drops more than recall for both AML and LogMap.
 This is actually observed - AML's precision and recall drop with respectively 4.6 percentage points and 0.9 percentage point around and LogMap's - with around 5.1 and 2.6 percentage points.
 The "Precision" value drops more than the value for "Negative Precision" as well for XMap but we do not observe changes for the precision and recall likely due to the very small number of requests.
 Another observation is connected to the values in the <b>"Precision Oracle" and "Recall Oracle"</b> columns.
 AML values are approximately constant, which means that the impact of the errors is linear - even though it does increase the number of queries with the error rate.
 The impact of the errors is also linear for XMap, the corresponding measures drop even less than AML's.
 ALIN has exactly the same precision according to the oracle; its recall drops slowly but consistently.
 In this case though, the drop in the recall is undoubtedly due to the fact that it decreases the number of queries with the error rate and thus captures less mappings in total.
 It should be safe to consider that the impact of the errors is also linear in the case of this tool. LogMap, as the last year, has a noticeable drop in both "Precision Oracle" and "Recall Oracle" which indicates a supralinear impact of errors.
 </p>

  <p>
  While the <b>size of the alignments</b> produced by LogMap and AML slightly increases with the increasing error rates, the size of the ALIN's alignment decreases.
  The same trend is observed for the number of total and distinct mappings requests that ALIN makes to the user - when the error rate increases the number of questions to the user decreases.

  The number of total and distinct mappings requests for AML increases with the error rate and the two values are equal for each tool across the different error rates, i.e., one user interaction contains one mapping and they do not ask the user the same question twice.
  ALIN on the other side combines several mappings in one request to the user.
  These two numbers for LogMap are same through the different error rates.
  Furthermore, different from last year, LogMap asks the relationships of ">", "<" and "=" in each request.
  This is why the number of distinct mappings (1164) is more than the number of total requests (388).
<!--
  This is also the reason why the precision and negative precision in last two columns from the table drop 57 percentage points and 12% around than 35% and 25% in 2016 respectively as the error rate increases.
-->
  The size of the alignment generated by XMap changes with less than a mapping on average, thus the tool is impacted by errors but on a very small scale.
  The number of the requests to the oracle does not change for XMap across the error rates.

  Another difference is observed in the ratio of correct to incorrect requests made to the oracle (<b>"True Positives" and "True Negatives"</b>).

  In the case of an all-knowing oracle, all the systems make more incorrect than correct requests. The ratios of correct and incorrect requests of ALIN, AML, LogMap and XMap are 0.88, 0.27, 0.32 and 0.17 respectively.
  AML, LogMap, XMap keep almost the same ratios in different error rates.
  However as the error rate increases, ALIN makes more correct requests with the ratios 1.02, 1.07, 1.12 in error rates 0.1, 0.2 and 0.3 respectively.
  </p>

 <p>
 For an interactive system the <b>time intervals</b> at which the user is involved in an interaction are important.
 Fig. 1 presents a comparison between the systems regarding the time periods at which the system presents a question to the user.
 Across the ten runs and different error rates the AML, LogMap and XMap mean requests intervals are around 1 and 0 ms respectively.
 The average of ALIN's mean intervals is 141 ms.
 While there are no significant deviations of this values for LogMap, for AML several outliers are observed, i.e., the interval between few of the AML requests took up to 400 ms, one even took up to 600 ms.
 The <b>run times</b> between the different error rates slightly decrease for ALIN while there is no significant change for LogMap, AML and XMap. AML, LogMap and XMap generate an alignment for less than a minute with LogMap performing twice as faster as AML.
</p>

 <p>
   The <b>take away</b> of these analyses is that all systems perform better with an all-knowing oracle than in the non-interactive Anatomy track in terms of F-measure.
   XMap benefits the least from the interaction with the oracle.
   ALIN performs  better for all error rates in the interactive evaluation than in the non-interactive one.
   It seems that ALIN is relying only on user interaction to generate non-trivial mappings.
   For AML all three measures (precision, recall and F-measure) are higher in the Interactive track with 0.0 and 0.1 error rates than in the non-interactive, for LogMap the recall in the 0.0 case is the same as its non-interactive recall and drops under it in the 0.1 error rate.
   The growth of the error rate impacts different measures in the different systems but noticeably the XMap performance is barely affected by user errors.
   The impact of the oracle errors is linear for ALIN, AML and XMap and supralinear for LogMap.
   The ratio of correct to incorrect questions to the oracle is different in the four systems and further changes with the increasing error rates which could be an indicator of different strategies to handle user errors.
 </p>

 <h4>Comparison to 2016</h4>
 <p>
  The participants this year are same as 2016. Among these, AML and LogMap have participated in the track since it has been established in 2013. AML, LogMap and XMap show similar results to the results from 2016 in terms of run time, alignment size, precision, recall, F-measure, request intervals and recall+.
  ALIN has an obvious increase in run time which is two times more than that in 2016.
  LogMap has the same number of total requests for all error rates which is lower than that in 2016 because it contains three different questions including ">", "<" and "=" relationships for each pair of concepts in one request.
  If the relationship of the two concepts is equal, the oracle will return "True Negative" for ">" and "<" relationships which is the reason why LogMap has a higher increase in the number of incorrect mappings.
   The change of F-measure with different error rates for each system is the same as the result in 2016.
   Similarly to the last year the impact of oracle's errors is linear for AML, ALIN, XMap and supralinear for LogMap.
  </p>
 <figure style="text-align:center;">
  <img src="anatomy_boxplot_per_error_rate_multiple_runs.png"><br/>
  <figcaption style="font-size:22px;margin: 40px 0 0 0;">Fig1. The Y axis depicts the time intervals between the requests to the user/oracle (whiskers: Q1-1,5IQR, Q3+1,5IQR, IQR=Q3-Q1).
  The labels under the system names show the average number of requests and the mean time between the requests for the ten runs.</figcaption>
 </figure>

 <h3>Conference Track</h3>
 <p>The four tables below present the results for the Conference dataset with four different error rates. The columns are described above at the beginning of the Results section. Fig. 2 shows the average requests intervals per task (21 tasks in total per run) between the questions to the user/oracle for the different systems and error rates for all tasks and the ten runs (the runs are depicted with different colors). The first number under the system name is the average number of requests and the second number is the average period of the average requests intervals for all tasks and runs. </p>
 <h4>Error Rate 0.0</h4>
 <div id="conference0"></div>
 <h4>Error Rate 0.1</h4>
 <div id="conference1"></div>
 <h4>Error Rate 0.2</h4>
 <div id="conference2"></div>
 <h4>Error Rate 0.3</h4>
 <div id="conference3"></div>

 <figure style="text-align:center;">
	<img src="conference_boxplot_per_error_rate_multiple_runs_aver_aver_Y_aver.png">
	<figcaption style="font-size:22px;margin: 40px 0 0 0;">Fig2. The Y axis depicts the average time between the requests per task in the Conference dataset (whiskers: Q1-1,5IQR, Q3+1,5IQR, IQR=Q3-Q1).
  The labels under the system names show the average number of requests and the mean time between the requests (calculated by taking the average of the average request intervals per task) for the ten runs and all tasks.</figcaption>
 </figure>

<p>
When systems are evaluated using <b>an all-knowing oracle</b>, AML, ALIN and LogMap present a higher F-measure than the one they obtained non-interactively.
ALIN shows the greatest improvement, almost doubling its F-measure, and also the highest F-measure in absolute terms when questioning the oracle.
AML and LogMap improved by 6 and 3.7 percentage points respectively.
The substantial improvement of ALIN is mostly supported by gains in recall (around 46 percentage points), whereas AML and LogMap show more balanced improvements.
XMap results are exactly the same as for the non-interactive run which is not very surprising given that it performs only four requests to the oracle for all 21 tasks and they are all for true negatives.
</p>
<p>
When an <b>error rate is introduced</b>, ALIN is the system that is more severely affected, loosing around 10, 8.5 and 7.7 percentage points in F-measure with each increase in error rate.
On the other hand, LogMap is resilient, with losses between 2 percentage points and 1 percentage point.
Similarly to the performance shown for the Anatomy dataset XMap is the most resilient and it is not impacted by the oracle errors.
In all tests with an error-producing oracle AML produced the highest F-measure.

When comparing the systems' performance across the different error rates with their non-interactive results we note that the precision drops earlier than the recall.
ALIN's recall is higher than its non-interactive results even in the 0.3 (its very low recall in the non-interactive track also contributes to this result) but the precision drops already for the smallest error rate.
AML's precision drops under the non-interactive results in the 0.2 case and the recall - in the 0.3.
LogMap's precision drops in the 0.3 case while its recall stays almost the same as the non-interactive results.

<!-- As indicated by the F-measure the decrease in the systems performance is significant when moving towards higher error rates especially for ALIN.  -->
The larger drop in <b>"Precision" than "Negative Precision"</b> values between the 0.0 and 0.3 error rates explains the larger decrease in precision (40, 20 and 9 percentage points) than the decrease in recall (17, 6 and 3 percentage points) for all three systems (ALIN, AML and LogMap).

The impact of oracle errors (<b>"Precision Oracle" and "Recall Oracle"</b> columns) on the systems' behaviour for the Conference dataset is similar to the impact of the errors on their behaviour for the Anatomy dataset - it is linear for ALIN and AML and supralinear for LogMap.
It is also linear for XMap, we observe almost unnoticable change for its "Recall Oracle" measure. <!-- With the increasing error rate the ALIN and AML values slowly increase and for LogMap clearly decrease which -->
</p>

<p>
The number of <b>total requests</b> posed by each system is different.
LogMap poses the lower number (82 for every test), while on average, AML poses around 284, whereas ALIN poses around 316 total requests.
As mentioned earlier XMap does make only 4 request for all 21 tasks.
The novel feature of analysing multiple mappings at once was apparently used by two systems, ALIN, which is querying one or more individual mappings for each interaction, and LogMap, which contains three mappings in one request.

Analysing the values for the <b>"True Positives" and "True Negatives"</b> we observe that the systems mainly ask the oracle about incorrect mappings (true negatives) - in the 0.0 case the ratio is 0.32, 0.21 and 0.25 for ALIN, AML and LogMap respectively.
This ratio stays the same in the 0.3 case for ALIN, slightly drops for LogMap to 0.24 but increases for AML to 0.34, i.e., with increasing the error rates AML starts to ask more positive questions.
XMap asks mainly about true negatives but with the increasing error rates starts asking about false positives.

<b>Time between requests</b> is low for all systems under 1ms, with LogMap showing an increase in variance for tasks where errors are introduced.
The total run times does not change between the error rates.
</p>
<p>
The <b>take away</b> of this analysis is that all systems, except XMap, perform better on all measures with an all-knowing oracle than in the non-interactive Conference track.
In the 0.1 error rate AML and LogMap still perform better on all measures, while ALIN's precision drops below its non-interactive precision.
Its F-measure however stays higher than the non-interactive value even in the 0.3 case due to the huge improvement in the recall.
XMap measures do not change across the error rates and are the same as for the non-interactive track.
</p>
<h4>Comparison to 2016</h4>
<p>
The participants this year are the same systems in 2016.
The variation trends with the respect to precision, recall and F-measure in each system between interactive and non-interactive are similar.
The influence due to the different error rates among these measures is also similar.
Regarding the requests to the oracle, LogMap has a decrease in total requests from 140 around in 2016 to 82 in 2017 which contributes the increase in the percentage of correct requests.
<!--
Since LogMap contains three questions which are ">", "<" and "=" relationships in each requests, it causes the higher decreases on precision and negative precision as the error rate increases.
-->
In terms of the time between requests, LogMap has a decrease from 1.9 ms on average in 2016 to less than 1ms in 2017. ALIN also has a decrease in time between requests from 1.48 on average in 2016 to 0.445 on average in 2017.
</p>



<h2>Discussion</h2>
<p>
While LogMap, XMap and AML make use of user interactions exclusively in the post-matching steps to filter their candidate mappings, ALIN can also add new candidate mappings to its initial set.
LogMap and AML both request feedback on only selected mapping candidates (based on their similarity patterns or their involvement in unsatisfiabilities). LogMap presents three mappings to the user while AML only presents one mapping at a time to the user.
XMap also presents one mapping at a time and asks mainly for true negatives.
ALIN also employs the new feature similar to LogMap - analysing several mappings simultaneously - and can present up to three mappings together to the user if a full entity name in a candidate mapping is the same as another entity name in another candidate mapping.<!-- It can add and remove candidate mappings as a result of the user interaction. -->
The difference between ALIN and LogMap is the latter contains three questions (">", "<" and "=") for every candidate mapping.
</p>
<p>
With the all-knowing oracle all systems, except XMap, perform better than their non-interactive results for both datasets.
Very noticeably XMap's performance does not change across the error rates and it also barely  benefits from the interaction with the oracle (and only for the Anatomy dataset).
Although systems' performance deteriorates when moving towards larger error rates there are still benefits from the user interaction - some of the systems' measures stay above their non-interactive values even for the larger error rates.
For the Anatomy dataset different measures are affected in the different systems in comparison to their non-interactive results; for the Conference dataset the precision drops under the non-interactive values faster than the recall.
The drop in precision and recall for all systems, except XMap, is larger for the Conference dataset than for the Anatomy dataset with the increasing error rates.
</p>
<p>
The impact of the oracle's errors is linear for ALIN, AML and XMap and supralinear for LogMap for both datasets.
One difference between the two datasets is the ratio of correct to incorrect requests to the oracle.
There is a clear trend for all three systems (except XMap) in the Conference dataset, the ratio stays around and under 0.5 for all error rates.
In the Anatomy dataset the ratio changes differently for each system.
</p>


<!-- <p>
This year is the first time we have considered a non-perfect domain expert, i.e., a domain expert which can provide wrong answers. As expected, the performance of the systems deteriorated with the increase of the error rate. However, an interesting observation is that the errors had different impact on different systems reflecting the different interactive strategies employed by the systems. In some cases erroneous answers from the oracle had the highest impact on the recall, in other cases on the precision, and in others still both measures were significantly affected. Also interesting is the fact that the impact of the errors was linear in some systems and supralinear in others, as reflected by their performance in relation to the Oracle alignment. A supralinear impact of the errors indicates that the system is making inferences from the user and thus deciding on the classification of multiple mapping candidates based on user feedback about only one mapping. This is an effective strategy for reducing the burden on the user, but alas leaves the matching system more susceptible to user errors. An extreme example of this is JarvisOM on the Anatomy dataset, as it uses an active-learning approach based on solely 7 user requests, and consequently is profoundly affected when faced with user errors given the size of the Anatomy dataset alignment. Curiously, this system behaves very differently in the Conference dataset, showing a linear impact of the errors, as in this case 7 requests (which is the average number it makes per task) represent a much more substantial portion of the Conference alignments (~50%) and thus leads to less inferences and consequently less impact of errors.
</p>
<p>
Apart from JarvisOM, the other systems all make use of user interactions exclusively in post-matching steps to filter their candidate mappings. LogMap and AML both request feedback on only selected mapping candidates (based on their similarity patterns or their involvement in unsatisfiabilities). By contrast, ServOMBI employs the user to validate all its mapping candidates (after two distinct matching stages), which corresponds to user validation rather than interactive matching. Consequently, it makes a much greater number of user requests than the other systems, and in being the system most dependent on the user, is also the one most affected by user errors.
</p>
<p>
With regard still to the number of user requests, it is interesting to note that both ServOMBI and LogMap generally increased the number of requests with the error, whereas AML and JarvisOM kept their number approximately constant. The increase is natural, as user errors can lead to more complex decision trees when interaction is used in filtering steps and inferences are drawn from the user feedback (such as during alignment repair) which leads to an increased number of subsequent requests. JarvisOM is not affected by this because it uses interaction during matching and makes a fixed 7-8 requests per matching task, whereas AML prevents it by employing a maximum query limit and stringent stopping criteria.
</p>-->
<p>
Two models for system response times are frequently used in the literature <a href="#references">(Dabrowski et al., 2011)</a>: Shneiderman and Seow take different approaches to categorize the response times.
Shneiderman takes task-centred view and sort out the response times in  four categories according to task complexity: typing, mouse movement (50-150 ms),
simple frequent tasks (1 s), common tasks (2-4 s) and complex tasks	(8-12 s). He suggests that the user is more tolerable to delays with
the growing complexity of the task at hand. Unfortunately no clear definition is given for how to define the task complexity.
The Seow's model looks at the problem from a user-centred perspective by considering the user expectations towards the execution of a task: instantaneous (100-200 ms),
immediate (0.5-1 s), continuous (2-5 s), captive (7-10 s);
Ontology alignment is a cognitively demanding task and can fall into the third or forth categories in both models.
In this regard the response times (request intervals as we call them above) observed with both the Anatomy and Conference dataset fall into the tolerable and acceptable response times, and even into the first categories, in both models.
The request intervals for both AML and LogMap stay around 1 ms for both datasets. ALIN's request intervals are 140 ms for the Anatomy dataset on average and less than 1 ms for the Conference dataset.
<!-- The same applies for the average requests intervals for the 6 tasks in the LargeBio dataset. -->
<!-- The average request intervals for the Conference dataset are lower than those discussed for the Anatomy dataset. --> It could be the case however that the user could not take advantage of very low response times because the task complexity may result in higher user response time
(analogically it measures the time the user needs to respond to the system after the system is ready).
</p>



<a name="references">
<h2>References</h2>
</a>

<p>
Zlatan Dragisic, Valentina Ivanova, Patrick Lambrix, Daniel Faria, Ernesto Jimenez-Ruiz and Catia Pesquita. "User validation in ontology alignment".
ISWC 2016. <a href="http://www.cs.ox.ac.uk/files/8268/oaei-user-interactive-track.pdf">[paper]</a>
<a href="../../interactive/oaei-user-interactive-track-extended-version.pdf">[technical report]</a>
</p>

<p>
Bernardo Cuenca Grau , Zlatan Dragisic , Kai Eckert & et al. "Results of the Ontology Alignment Evaluation Initiative 2013"
OM 2013. <a href="http://oaei.ontologymatching.org/2013/results/oaei2013.pdf">[pdf]</a>
</p>
<p>
Heiko Paulheim, Sven Hertling, Dominique Ritze. "Towards Evaluating Interactive Ontology Matching Tools".
ESWC 2013. <a href="http://eswc-conferences.org/sites/default/files/papers2013/paulheim.pdf">[pdf]</a>
</p>

<p>
Ernesto Jimenez-Ruiz, Bernardo Cuenca Grau, Yujiao Zhou, Ian Horrocks. "Large-scale Interactive Ontology Matching:
Algorithms and Implementation". ECAI 2012. <a href="https://www.cs.ox.ac.uk/files/4801/LogMap_ecai2012.pdf">[pdf]</a>
<p>

<p>
 Valentina Ivanova, Patrick Lambrix, Johan &Aring;berg. "Requirements for and evaluation of user support for large-scale ontology alignment". ESWC 2015.
<a href="http://link.springer.com/chapter/10.1007%2F978-3-319-18818-8_1">[publisher page]</a>
</p>

<p>
 Pavel Shvaiko, J&eacute;r&ocirc;me Euzenat. "Ontology matching: state of the art and future challenges". Knowledge and Data Engineering 2013.
 <a href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6104044&tag=1">[publisher page]</a>
</p>

<p>
 Jim Dabrowski, Ethan V. Munson. "40 years of searching for the best computer system response time". Interacting with Computers 2011.
 <a href="http://www.sciencedirect.com/science/article/pii/S0953543811000579">[publisher page]</a>
</p>

<h2>Contact</h2>

<p>This track is currently organized by
<a href="http://www.ida.liu.se/department/contact/contactcard.en.shtml?zladr41">Zlatan Dragisic</a>,
<a href="http://xldb.fc.ul.pt/~dfaria/">Daniel Faria</a>,
<a href="http://www.ida.liu.se/department/contact/contactcard.en.shtml?valiv">Valentina Ivanova</a>,
<a href="http://www.cs.ox.ac.uk/isg/people/ernesto.jimenez/">Ernesto Jimenez Ruiz</a>,
<a href="http://www.ida.liu.se/~patla/index.shtml">Patrick Lambrix</a>,
<a href="http://www.ida.liu.se/department/contact/contactcard.en.shtml?huali50">Huanyu Li</a>,
and
<a href="http://www.di.fc.ul.pt/~catiapesquita/">Catia Pesquita</a>.

If you have any problems working with the ontologies or any suggestions
related to this track, feel free to write an email to ernesto
[at] cs [.] ox [.] ac [.] uk or ernesto [.] jimenez [.] ruiz [at] gmail [.] com</p>

<br />

<h2>Acknowledgements</h2>

<p>We thank Dominique Ritze and Heiko Paulheim, the organisers of
the <a href="../../2013/interactive/">2013</a> and <a href="../../2014/interactive/">2014</a> editions
of this track, who were very helpful in the setting up of the 2015 edition.
</p>

<p>
The track is partially supported by the <a href="http://www.optique-project.eu/">Optique</a> project.
</p>

<!-- <h2>References</h2>
Jimnez-Ruiz, E., Grau, B. C., & Yujiao, Z. (2011). LogMap 2.0: towards logic-based, scalable and interactive ontology matching. Proceedings of the 4th International Workshop on Semantic Web Applications and Tools for the Life Sciences, (S. 45-46).<br/><br/>
Paulheim, H., Hertling, S., & Ritze, D. (2013). Towards Evaluating Interactive Ontology Matching Tools. Proceedings of the 10th Extended Semantic Web Conference, (S. 31-45).<br/><br/>
Ritze, D. & Paulheim, H. (2011). Towards an automatic parameterization of ontology matching tools based on example mappings. Proceesings of the 6th Ontology Matching Workshop, (S.37-48).<br/><br/>
 -->
</div>
</body>
</html>
